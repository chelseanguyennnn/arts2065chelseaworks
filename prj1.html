<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Project 1 | Chelsea Nguyen</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <header>
    <nav>
      <ul class="nav">
        <li><a href="index.html#about">About Me</a></li>
        <li><a href="index.html#projects" class="active">Projects</a></li>
        <li><a href="index.html#arts2065">ARTS2065</a></li>
      </ul>
    </nav>
    <h1>Speculative History of Artificial Intelligence</h1>
  </header>
  <main>
    <section class="project1-section">
      <details class="dropbox-h2">
        <summary><h2>Memory Travel</h2></summary>
        <div class="video-box" style="margin-bottom:1.5rem;">
          <iframe width="600" height="338" src="https://www.youtube.com/embed/DmLVu9Wh2OI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <div class="project1-desc">
          <p><strong>Set in the year 2100</strong>, this speculative narrative follows a girl trapped inside a glass room who speaks through a system called <em>Memory Travel</em>, sending messages to her past self in 2025.</p>
          <p>Through her fragmented memories, we witness humanity’s evolution alongside AI, from the first neural implants and the rise of artificial general intelligence to the creation of “superhuman” societies divided by digital class and control.</p>
          <p>As AI begins to rewrite human memory, love, and identity, the line between mind and machine dissolves. In this world where thoughts can be edited and dreams can be sold, the story questions <strong>what remains truly human when progress costs us our very sense of self</strong>.</p>
        </div>
      </details>
      <details class="dropbox-h2">
        <summary><h2>Essay</h2></summary>
        <div class="essay-content">
          <p><strong>The recent story of Yang, a Chinese Ph.D. student expelled from the University of Minnesota for allegedly using ChatGPT during an exam, highlights the growing tension between academic integrity and the ethical use of AI tools.</strong> Yang argues in his lawsuit against his university that there is no concrete evidence he used large language models (LLMs) to cheat and that he merely employed them to overcome language-related disadvantages faced by many international students (Ward 2025). This case occurred in August 2024, only two years after LLMs gained popularity, but it raises a crucial question about the regulations needed for LLM use: <em>Where should we draw the ethical boundary in their academic use?</em></p>

          <p>While LLMs can serve as valuable tools for learning and support, I believe that humans must regulate the new technology to preserve the role of human creativity, critical thinking, and fairness in education. Therefore, this essay will explore the thinking capacities of LLMs in contrast to human cognition, examine their dual impact on students’ intellectual development, and use Yang’s case to reflect on how AI can be ethically integrated into academic life.</p>

          <h3>Are chatbots, such as ChatGPT, capable of genuine understanding?</h3>
          <p>To assess whether LLMs such as ChatGPT possess genuine understanding, we must first consider how they function. ChatGPT predicts the likelihood of a token (such as a word or phrase) based on surrounding context (Lindebaum & Fleming 2023). It processes enormous datasets that include text, images, and code to generate coherent responses (Ball 2023). Yet it operates as an “acontextual linguistic algorithm,” meaning it produces language without grasping its meaning (Lindebaum & Fleming 2023).</p>

          <p>Although originally developed for human-like interaction, ChatGPT has evolved to generate creative content such as poems, essays, and even behavioural mimicry (Essel et al., 2024). However, it remains unreliable as a source of knowledge, often producing hallucinations, false references, and inaccuracies (Essel et al. 2024; Narayanan & Kapoor 2024).</p>

          <p>This raises the more profound question: does ChatGPT understand the words it produces? Researchers are divided. While Google’s chief AI scientist claims that LLMs may comprehend the ideas behind their language, critic Gary Marcus argues that they merely replicate surface-level patterns, stating that “literally everything the system says is bullshit” (Ball 2023).</p>

          <p>Current evidence supports the view that ChatGPT lacks intuition, contextual judgement, and the ability to verify truth from falsehood (Lindebaum & Fleming 2023). For example, Microsoft researchers asked ChatGPT to write a proof of the infinitude of prime numbers in the style of Shakespeare. Although the result was linguistically impressive, it showed imitation rather than true comprehension (Ball 2023). The experiment illustrates how LLMs can convincingly manipulate language without understanding its meaning. Unlike humans, ChatGPT cannot grasp concepts such as love, illness, or warmth, because these require emotional depth and lived experience, not just textual pattern recognition.</p>

          <p><strong>In summary, while ChatGPT excels at producing fluent and persuasive text, it lacks authentic understanding. This distinction between simulation and cognition underscores why students and educators must use LLMs thoughtfully. Overreliance on such tools risks diminishing our capacity for critical and original thinking. Therefore, making clear ethical boundaries is not just a desirable but an essential act.</strong></p>

          <h3>The impact of ChatGPT on cognitive development.</h3>
          <p>New technologies inevitably shape the way people think and learn, often with mixed consequences. As generative AI like LLMs continues to develop, it is essential to remain cautious about its long-term impact on students' intellectual growth. Yet, with proper guidance, LLMs can enhance students' critical thinking skills, even amid concerns about their drawbacks.</p>

          <p>According to Narayanan and Kapoor (2024), generative AI does not pose a greater threat to critical thinking than calculators did when they were introduced. When calculators became widespread, society accepted a trade-off: sacrificing manual computation skills in favour of faster problem-solving, advanced exploration, and greater accessibility. Similarly, a study by Essel et al. (2024) found that ChatGPT promotes critical thinking by encouraging students to ask questions and engage in deeper reflection. While LLMs may lose some originality in writing, they can also enhance access to writing resources for a more diverse, global audience.</p>

          <p>However, LLMs also present significant challenges to students' creativity. The algorithms behind tools like ChatGPT are inherently flawed, biassed, and sometimes misleading, reinforcing stereotypes and prejudices (Gillespie 2014). Research suggests that people are more likely to absorb and trust AI-generated biases because they assume that machine-generated information is objective (Leffer 2023). Additionally, the convenience of ready-made answers can discourage students from seeking diverse perspectives, leading to over-reliance on AI as a "verified" source. This dependence not only limits independent thought but also risks stifling creativity and more profound analysis.</p>

          <p>While LLMs can expand access to knowledge and strengthen critical thinking, they also risk diminishing students' originality and creativity. Striking a balance between leveraging AI's potential and preserving essential human skills is crucial to the ethical integration of AI in academia.</p>

          <h3>Debate the ethical use of ChatGPT in academic writing.</h3>
          <p>Yang’s case highlights the complex intersection between academic misconduct and the ethical use of LLMs in education. Similar to any transformative technology, LLMs are becoming increasingly integrated into everyday life, making prohibition both unrealistic and ineffective. Instead of banning their use, I believe it is time for educators to find responsible ways to incorporate these tools while addressing potential risks.</p>

          <p>Yang argued that the university’s method of detecting AI involvement was flawed and biassed, particularly against non-native English speakers. Although some may doubt Yang’s arguments, this situation raises a critical question: should a student face severe consequences when the evidence is inconclusive and the detection process lacks transparency? It is also the university’s responsibility to design assessments that anticipate how students might engage with emerging technologies.</p>

          <p>A more constructive approach would be to promote transparency in student use of AI. For example, in my economics course at UNSW, students are required to submit an AI Transparency Report with their assignments. This policy reframes LLMs not as shortcuts but as tools that, when used thoughtfully, can enhance learning. It encourages students to evaluate AI-generated content critically and to reflect on ethical academic practices.</p>

          <p><strong>In conclusion, how we choose to engage with LLMs today will influence how they shape our academic and professional environments in the future. The challenge lies not in whether we should use them, but in how to do so ethically, ensuring that we remain in control of the technology rather than being controlled by it.</strong></p>

          <h3>Bibliography</h3>
          <ul class="essay-bibliography">
            <li>Ball, P. 2023, Can Machines Think?, Prospect Magazine, Available at: <a href="https://www.prospectmagazine.co.uk/ideas/technology/ai/62131/can-machines-think" target="_blank">link</a> [Accessed 19 Mar 2025].</li>
            <li>Benasayag, M. 2023, Humans, not machines, create meaning, UNESCO Courier, Available at: <a href="https://courier.unesco.org/en/articles/humans-not-machines-create-meaning" target="_blank">link</a> [Accessed 19 Mar 2025].</li>
            <li>Bergstrom, C.T. & Ogbunu, C.B. 2023, ChatGPT Isn’t ‘Hallucinating.’ It’s Bullshitting., Undark Magazine, Available at: <a href="https://undark.org/2023/04/06/chatgpt-isnt-hallucinating-its-bullshitting/" target="_blank">link</a> [Accessed 19 Mar 2025].</li>
            <li>Clune, M.W. 2023, ‘AI Means Professors Need to Raise Their Grading Standards’, The Chronicle of Higher Education, 14 September, Available at: <a href="https://www.chronicle.com/article/ai-means-professors-need-to-raise-their-grading-standards" target="_blank">link</a> [Accessed 19 Mar 2025].</li>
            <li>Essel, H.B., Vlachopoulos, D., Essuman, A.B. & Amankwa, J.O., 2024. ‘ChatGPT effects on cognitive skills of undergraduate students: Receiving instant responses from AI-based conversational large language models (LLMs)’, Computers and Education: Artificial Intelligence, 6, p.100198.</li>
            <li>Gillespie, T., 2014, The relevance of algorithms, Media Technologies: Essays on Communication, Materiality, and Society, 167(2014), p.167.</li>
            <li>Helmore, E. 2023, ‘“We are a little bit scared”: OpenAI CEO warns of risks of artificial intelligence’, The Guardian, 17 March, Available at: <a href="https://www.theguardian.com/technology/2023/mar/17/openai-sam-altman-artificial-intelligence-warning-gpt4" target="_blank">link</a> [Accessed 19 Mar 2025].</li>
            <li>Leffer, L. 2023, Humans Absorb Bias from AI—And Keep It after They Stop Using the Algorithm, Scientific American, Available at: <a href="https://www.scientificamerican.com/article/humans-absorb-bias-from-ai-and-keep-it-after-they-stop-using-the-algorithm/" target="_blank">link</a> [Accessed 19 Mar 2025].</li>
            <li>Lindebaum, D. & Fleming, P. 2024, ‘ChatGPT Undermines Human Reflexivity, Scientific Responsibility and Responsible Management Research’, British Journal of Management, vol. 35, no. 2, pp. 566–575.</li>
            <li>Narayanan, A. & Kapoor, S. 2024, AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference, Princeton University Press, Princeton, New Jersey.</li>
            <li>Royer, C., 2024, ‘Outsourcing humanity? ChatGPT, critical thinking, and the crisis in higher education’, Studies in Philosophy and Education, 43(5), pp.479-497.</li>
            <li>Sahakian, B.J., Langley, C. & Leong, V. 2021, What is cognitive flexibility and how does it help us think?, World Economic Forum, Available at: <a href="https://www.weforum.org/stories/2021/06/cognitive-flexibility-thinking-iq-intelligence/" target="_blank">link</a> [Accessed 19 Mar 2025].</li>
            <li>Ward, H. 2025, ‘Ph.D. student sues UMN, files human rights complaint after AI plagiarism expulsion’, The Minnesota Daily, Available at: <a href="https://mndaily.com/292797/campus-administration/ph-d-student-sues-umn-files-human-rights-complaint-after-ai-plagiarism-expulsion/" target="_blank">link</a> [Accessed 19 Mar 2025].</li>
          </ul>
        </div>
      </details>
    </section>
  </main>
  <footer>
    &copy; 2025 Chelsea Nguyen. All rights reserved.
  </footer>
</body>
</html>
